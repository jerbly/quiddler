{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference testing function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import json \n",
    "\n",
    "def get_base64_encoded_image(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "def test_inf(endpoint, key=None):\n",
    "    input_json = json.dumps({\n",
    "        \"n_cards\": 5,\n",
    "        \"images\": True,\n",
    "        \"hand\": get_base64_encoded_image('/home/jeremy/Documents/data/quiddler/test/IMG_4903.jpg'),\n",
    "        \"deck\": get_base64_encoded_image('/home/jeremy/Documents/data/quiddler/train/A_1.jpg')\n",
    "    })\n",
    "    headers = { 'Content-Type':'application/json' }\n",
    "    if key:\n",
    "        headers['Authorization'] = f'Bearer {key}' # Only difference to local is to send the auth key\n",
    "    predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "    p = predictions.json()\n",
    "    print(str(p)+'\\n')\n",
    "    print(f'Hand:     {p[0]}')\n",
    "    print(f'Deck:     {p[1]}')\n",
    "    if p[2]:\n",
    "        play = p[2][0]\n",
    "        print(f'Score:    {play[0]}')\n",
    "        print(f'Complete: {play[1]}')\n",
    "        print(f'Words:    {[c[0] for c in play[2]]}')\n",
    "        print(f'Pick up:  {p[2][1]}')\n",
    "        print(f'Drop:     {p[2][2]}\\n')\n",
    "    else:\n",
    "        print('No possible play for these cards')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print(f'Azure ML version: {azureml.core.VERSION}, Workspace: {ws.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the inference environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice, AciWebservice, Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = Environment(name=\"quiddler-inference-env\")\n",
    "myenv.inferencing_stack_version='latest'\n",
    "myenv.docker.enabled = True\n",
    "myenv.docker.base_image = None\n",
    "\n",
    "# Specify docker steps as a string.\n",
    "# see https://github.com/Azure/AzureML-Containers\n",
    "# Needed to add libgl1-mesa-dev to resolve:\n",
    "#  ImportError: libGL.so.1: cannot open shared object file: No such file or directory\n",
    "dockerfile = r'''\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
    "RUN apt-get install -y libgl1-mesa-dev\n",
    "'''\n",
    "myenv.docker.base_dockerfile = dockerfile\n",
    "\n",
    "conda_dep = CondaDependencies(conda_dependencies_file_path='inference-env.yml')\n",
    "myenv.python.conda_dependencies=conda_dep\n",
    "\n",
    "# Configure the scoring environment\n",
    "inference_config = InferenceConfig(source_directory = 'source',\n",
    "                                   entry_script=\"score.py\",\n",
    "                                   environment=myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['quiddler_model']\n",
    "print(f'Model: {model.name}, Version: {model.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployment_config = LocalWebservice.deploy_configuration()\n",
    "\n",
    "service_name = \"quiddler-service\"\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload the service to pick up changes in the source directory\n",
    "service.reload(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_endpoint = service.scoring_uri\n",
    "print(local_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inf(local_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()\n",
    "print(\"Service deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Azure Container Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACI Service\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, enable_app_insights=True)\n",
    "\n",
    "service_name = \"quiddler-service\"\n",
    "\n",
    "aci_service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_endpoint = aci_service.scoring_uri\n",
    "print(aci_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inf(aci_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy into AKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.compute import AksCompute\n",
    "\n",
    "service_name = \"quiddler-service2\"\n",
    "aks_target = AksCompute(ws,\"jb-inf\")\n",
    "# If deploying to a cluster configured for dev/test, ensure that it was created with enough\n",
    "# cores and memory to handle this deployment configuration. Note that memory is also used by\n",
    "# things such as dependencies and AML components.\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "aks_service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, aks_target)\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)\n",
    "#print(aks_service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary, secondary = aks_service.get_keys()\n",
    "aks_endpoint = aks_service.scoring_uri\n",
    "print(aks_endpoint)\n",
    "print(primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inf(aks_endpoint, primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
